{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhongchun/ai-base/blob/main/Training_an_Image_Classification_Model_in_PyTorch_(demo_v0_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKU8kmSs65xv"
      },
      "source": [
        "# ***Training an Image Classification Model in PyTorch***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zK9b4yiMRzB"
      },
      "source": [
        "#### Deep Lake enables users to manage their data more easily so they can train better ML models. This tutorial shows you how to train a simple image classification model while streaming data from a Deep Lake dataset stored in the cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UseHLcoRIYz"
      },
      "source": [
        "## Install Deep Lake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5mOffq5RN-T"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip3 install deeplake\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wGo53ndMTCB"
      },
      "source": [
        "## Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52h9xKujOJFs"
      },
      "source": [
        "The first step is to load the dataset for training. This tutorial uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset that has already been converted into Deep Lake format. It is a simple image classification dataset that categorizes images by clothing type (trouser, shirt, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neD2jhKDQ5WD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14482c10-3a15-469a-888b-21e743c4f5b0"
      },
      "source": [
        "import deeplake\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, time\n",
        "import torch\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# Connect to the training and testing datasets\n",
        "ds_train = deeplake.load('hub://activeloop/fashion-mnist-train')\n",
        "ds_test = deeplake.load('hub://activeloop/fashion-mnist-test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening dataset in read-only mode as you don't have write permissions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "-"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/fashion-mnist-train\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hub://activeloop/fashion-mnist-train loaded successfully.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "-"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening dataset in read-only mode as you don't have write permissions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\\"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/fashion-mnist-test\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\\"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hub://activeloop/fashion-mnist-test loaded successfully.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r\r\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np5fIbViHlCu"
      },
      "source": [
        "The next step is to define a transformation function that will process the data and convert it into a format that can be passed into a deep learning model. In this particular example, `torchvision.transforms` is used as a part of the transformation pipeline that performs operations such as normalization and image augmentation (rotation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqdWgumwQ1d6"
      },
      "source": [
        "tform = transforms.Compose([\n",
        "    transforms.RandomRotation(20), # Image augmentation\n",
        "    transforms.ToTensor(), # Must convert to pytorch tensor for subsequent operations to run\n",
        "    transforms.Normalize([0.5], [0.5]),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGmWr44PIQMk"
      },
      "source": [
        "You can now create a pytorch dataloader that connects the Deep Lake dataset to the PyTorch model using the provided method `ds.pytorch()`. This method automatically applies the transformation function and takes care of random shuffling (if desired). The `num_workers` parameter can be used to parallelize data preprocessing, which is critical for ensuring that preprocessing does not bottleneck the overall training workflow.\n",
        "\n",
        "The `transform` input is a dictionary where the `key` is the tensor name and the `value` is the transformation function that should be applied to that tensor. If a specific tensor's data does not need to be returned, it should be omitted from the keys. If a tensor's data does not need to be modified during preprocessing, the transformation function is set as `None`, which will convert the input data to a torch tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeiU4LobROdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef24514-ff40-4bec-ae69-15ae4035b414"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Since torchvision transforms expect PIL images, we use the 'pil' decode_method for the 'images' tensor. This is much faster than running ToPILImage inside the transform\n",
        "train_loader = ds_train.pytorch(num_workers = 0, shuffle = True, transform = {'images': tform, 'labels': None}, batch_size = batch_size, decode_method = {'images': 'pil'})\n",
        "test_loader = ds_test.pytorch(num_workers = 0, transform = {'images': tform, 'labels': None}, batch_size = batch_size, decode_method = {'images': 'pil'})\n",
        "\n",
        "type(train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snt5b6qwIZQ_"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5LZrDU4I1GO"
      },
      "source": [
        "This tutorial uses a pre-trained [ResNet18](https://pytorch.org/hub/pytorch_vision_resnet/) neural network from the torchvision.models module, converted to a single-channel network for grayscale images.\n",
        "\n",
        "Training is run on a GPU if possible. Otherwise, run on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "Ma1cTn02Trjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d34fb6c-3e54-4016-e5aa-a4ae0aca983a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRBRaROLROUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d49fee0-190c-478a-a601-2b8caad25fb1"
      },
      "source": [
        "# Use a pre-trained ResNet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Convert model to grayscale\n",
        "model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Update the fully connected layer based on the number of classes in the dataset\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(ds_train.labels.info.class_names))\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Specity the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 113MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sVS5lTFI-gZ"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V65Xr8aBJCUL"
      },
      "source": [
        "Helper functions for training and testing the model are defined. Note that the output from Deep Lake's PyTorch dataloader is fed into the model just like data from ordinary PyTorch dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6cQJkHeJGtk"
      },
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Zero the performance stats for each epoch\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for i, data in enumerate(data_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs = data['images']\n",
        "        labels = torch.squeeze(data['labels'])\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        # Print performance statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 0:    # print every 10 batches\n",
        "            batch_time = time.time()\n",
        "            speed = (i+1)/(batch_time-start_time)\n",
        "            print('[%5d] loss: %.3f, speed: %.2f, accuracy: %.2f %%' %\n",
        "                  (i, running_loss, speed, accuracy))\n",
        "\n",
        "            running_loss = 0.0\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "\n",
        "def test_model(model, data_loader, device):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    start_time = time.time()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(data_loader):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs = data['images']\n",
        "            labels = torch.squeeze(data['labels'])\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs.float())\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        print('Finished Testing')\n",
        "        print('Testing accuracy: %.1f %%' %(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQWzFjzLJINu"
      },
      "source": [
        "**The model and data are ready for training 🚀!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMhm4VjDRf7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba547b0-abad-46be-bd47-40bfcc2f3e19"
      },
      "source": [
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "    print(\"------------------ Training Epoch {} ------------------\".format(epoch+1))\n",
        "    train_one_epoch(model, optimizer, train_loader, device)\n",
        "\n",
        "    test_model(model, test_loader, device)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------ Training Epoch 1 ------------------\n",
            "[    0] loss: 2.470, speed: 0.37, accuracy: 9.38 %\n",
            "[   10] loss: 25.037, speed: 1.68, accuracy: 13.12 %\n",
            "[   20] loss: 21.850, speed: 2.02, accuracy: 24.06 %\n",
            "[   30] loss: 20.481, speed: 2.05, accuracy: 30.94 %\n",
            "[   40] loss: 18.530, speed: 2.17, accuracy: 39.69 %\n",
            "[   50] loss: 16.977, speed: 2.25, accuracy: 49.69 %\n",
            "[   60] loss: 16.831, speed: 2.23, accuracy: 45.00 %\n",
            "[   70] loss: 14.508, speed: 2.28, accuracy: 54.38 %\n",
            "[   80] loss: 15.085, speed: 2.32, accuracy: 51.25 %\n",
            "[   90] loss: 14.175, speed: 2.33, accuracy: 54.38 %\n",
            "[  100] loss: 13.567, speed: 2.32, accuracy: 50.94 %\n",
            "[  110] loss: 12.379, speed: 2.35, accuracy: 60.94 %\n",
            "[  120] loss: 13.290, speed: 2.37, accuracy: 54.38 %\n",
            "[  130] loss: 11.994, speed: 2.35, accuracy: 58.12 %\n",
            "[  140] loss: 11.554, speed: 2.37, accuracy: 61.88 %\n",
            "[  150] loss: 12.972, speed: 2.39, accuracy: 55.00 %\n",
            "[  160] loss: 12.506, speed: 2.37, accuracy: 60.62 %\n",
            "[  170] loss: 10.631, speed: 2.38, accuracy: 64.69 %\n",
            "[  180] loss: 10.917, speed: 2.39, accuracy: 63.44 %\n",
            "[  190] loss: 10.886, speed: 2.39, accuracy: 62.50 %\n",
            "[  200] loss: 10.599, speed: 2.38, accuracy: 64.69 %\n",
            "[  210] loss: 10.417, speed: 2.39, accuracy: 65.94 %\n",
            "[  220] loss: 11.089, speed: 2.41, accuracy: 59.69 %\n",
            "[  230] loss: 9.569, speed: 2.39, accuracy: 66.25 %\n",
            "[  240] loss: 10.204, speed: 2.40, accuracy: 64.38 %\n",
            "[  250] loss: 9.786, speed: 2.41, accuracy: 66.88 %\n",
            "[  260] loss: 8.384, speed: 2.40, accuracy: 70.62 %\n",
            "[  270] loss: 9.759, speed: 2.41, accuracy: 67.19 %\n",
            "[  280] loss: 9.655, speed: 2.42, accuracy: 69.69 %\n",
            "[  290] loss: 10.309, speed: 2.42, accuracy: 63.12 %\n",
            "[  300] loss: 8.416, speed: 2.41, accuracy: 69.69 %\n",
            "[  310] loss: 9.320, speed: 2.42, accuracy: 65.62 %\n",
            "[  320] loss: 9.429, speed: 2.43, accuracy: 65.31 %\n",
            "[  330] loss: 8.668, speed: 2.41, accuracy: 65.31 %\n",
            "[  340] loss: 8.487, speed: 2.42, accuracy: 70.00 %\n",
            "[  350] loss: 9.348, speed: 2.43, accuracy: 67.81 %\n",
            "[  360] loss: 9.439, speed: 2.42, accuracy: 64.06 %\n",
            "[  370] loss: 9.655, speed: 2.42, accuracy: 65.00 %\n",
            "[  380] loss: 8.983, speed: 2.43, accuracy: 68.44 %\n",
            "[  390] loss: 8.210, speed: 2.43, accuracy: 74.38 %\n",
            "[  400] loss: 9.396, speed: 2.42, accuracy: 64.69 %\n",
            "[  410] loss: 8.860, speed: 2.43, accuracy: 66.56 %\n",
            "[  420] loss: 8.296, speed: 2.44, accuracy: 69.69 %\n",
            "[  430] loss: 9.607, speed: 2.43, accuracy: 65.00 %\n",
            "[  440] loss: 8.006, speed: 2.43, accuracy: 72.50 %\n",
            "[  450] loss: 9.291, speed: 2.44, accuracy: 67.81 %\n",
            "[  460] loss: 7.922, speed: 2.44, accuracy: 73.75 %\n",
            "[  470] loss: 8.438, speed: 2.43, accuracy: 70.31 %\n",
            "[  480] loss: 8.589, speed: 2.44, accuracy: 72.50 %\n",
            "[  490] loss: 8.319, speed: 2.44, accuracy: 70.31 %\n",
            "[  500] loss: 7.632, speed: 2.43, accuracy: 72.50 %\n",
            "[  510] loss: 7.762, speed: 2.44, accuracy: 73.44 %\n",
            "[  520] loss: 9.433, speed: 2.44, accuracy: 68.75 %\n",
            "[  530] loss: 9.294, speed: 2.43, accuracy: 66.88 %\n",
            "[  540] loss: 7.432, speed: 2.44, accuracy: 73.12 %\n",
            "[  550] loss: 9.398, speed: 2.44, accuracy: 67.50 %\n",
            "[  560] loss: 7.645, speed: 2.44, accuracy: 71.25 %\n",
            "[  570] loss: 7.476, speed: 2.44, accuracy: 70.31 %\n",
            "[  580] loss: 9.572, speed: 2.44, accuracy: 65.94 %\n",
            "[  590] loss: 7.799, speed: 2.44, accuracy: 70.62 %\n",
            "[  600] loss: 8.651, speed: 2.44, accuracy: 65.62 %\n",
            "[  610] loss: 7.413, speed: 2.44, accuracy: 71.56 %\n",
            "[  620] loss: 8.087, speed: 2.44, accuracy: 72.50 %\n",
            "[  630] loss: 8.142, speed: 2.44, accuracy: 70.62 %\n",
            "[  640] loss: 8.952, speed: 2.44, accuracy: 70.62 %\n",
            "[  650] loss: 8.146, speed: 2.44, accuracy: 69.06 %\n",
            "[  660] loss: 7.991, speed: 2.45, accuracy: 70.94 %\n",
            "[  670] loss: 8.884, speed: 2.44, accuracy: 69.38 %\n",
            "[  680] loss: 7.656, speed: 2.44, accuracy: 72.81 %\n",
            "[  690] loss: 8.409, speed: 2.45, accuracy: 70.62 %\n",
            "[  700] loss: 6.912, speed: 2.44, accuracy: 75.00 %\n",
            "[  710] loss: 7.513, speed: 2.44, accuracy: 74.38 %\n",
            "[  720] loss: 7.561, speed: 2.45, accuracy: 73.75 %\n",
            "[  730] loss: 8.341, speed: 2.45, accuracy: 68.12 %\n",
            "[  740] loss: 8.112, speed: 2.44, accuracy: 67.19 %\n",
            "[  750] loss: 6.747, speed: 2.45, accuracy: 75.94 %\n",
            "[  760] loss: 7.806, speed: 2.45, accuracy: 69.38 %\n",
            "[  770] loss: 7.206, speed: 2.44, accuracy: 72.50 %\n",
            "[  780] loss: 7.073, speed: 2.45, accuracy: 72.81 %\n",
            "[  790] loss: 7.672, speed: 2.45, accuracy: 71.56 %\n",
            "[  800] loss: 7.378, speed: 2.45, accuracy: 71.56 %\n",
            "[  810] loss: 8.739, speed: 2.45, accuracy: 70.62 %\n",
            "[  820] loss: 7.107, speed: 2.45, accuracy: 75.94 %\n",
            "[  830] loss: 7.507, speed: 2.45, accuracy: 73.12 %\n",
            "[  840] loss: 7.119, speed: 2.44, accuracy: 75.62 %\n",
            "[  850] loss: 7.333, speed: 2.45, accuracy: 73.12 %\n",
            "[  860] loss: 8.119, speed: 2.45, accuracy: 70.31 %\n",
            "[  870] loss: 7.575, speed: 2.44, accuracy: 71.25 %\n",
            "[  880] loss: 7.776, speed: 2.44, accuracy: 71.25 %\n",
            "[  890] loss: 7.823, speed: 2.44, accuracy: 72.81 %\n",
            "[  900] loss: 7.604, speed: 2.44, accuracy: 73.44 %\n",
            "[  910] loss: 7.065, speed: 2.44, accuracy: 73.75 %\n",
            "[  920] loss: 7.661, speed: 2.44, accuracy: 71.88 %\n",
            "[  930] loss: 6.996, speed: 2.44, accuracy: 73.75 %\n",
            "[  940] loss: 7.711, speed: 2.44, accuracy: 75.31 %\n",
            "[  950] loss: 7.879, speed: 2.44, accuracy: 70.00 %\n",
            "[  960] loss: 7.426, speed: 2.44, accuracy: 73.12 %\n",
            "[  970] loss: 7.265, speed: 2.44, accuracy: 74.69 %\n",
            "[  980] loss: 7.601, speed: 2.44, accuracy: 74.38 %\n",
            "[  990] loss: 6.366, speed: 2.44, accuracy: 76.56 %\n",
            "[ 1000] loss: 7.700, speed: 2.44, accuracy: 69.38 %\n",
            "[ 1010] loss: 7.521, speed: 2.44, accuracy: 70.62 %\n",
            "[ 1020] loss: 8.066, speed: 2.44, accuracy: 70.94 %\n",
            "[ 1030] loss: 6.594, speed: 2.44, accuracy: 77.50 %\n",
            "[ 1040] loss: 7.347, speed: 2.44, accuracy: 72.19 %\n",
            "[ 1050] loss: 7.776, speed: 2.44, accuracy: 70.94 %\n",
            "[ 1060] loss: 6.713, speed: 2.44, accuracy: 74.38 %\n",
            "[ 1070] loss: 7.510, speed: 2.44, accuracy: 73.12 %\n",
            "[ 1080] loss: 6.984, speed: 2.44, accuracy: 71.25 %\n",
            "[ 1090] loss: 6.766, speed: 2.44, accuracy: 74.38 %\n",
            "[ 1100] loss: 7.997, speed: 2.44, accuracy: 69.06 %\n",
            "[ 1110] loss: 6.510, speed: 2.44, accuracy: 77.81 %\n",
            "[ 1120] loss: 7.084, speed: 2.45, accuracy: 74.38 %\n",
            "[ 1130] loss: 6.701, speed: 2.45, accuracy: 75.62 %\n",
            "[ 1140] loss: 6.836, speed: 2.44, accuracy: 77.50 %\n",
            "[ 1150] loss: 7.375, speed: 2.45, accuracy: 73.75 %\n",
            "[ 1160] loss: 6.559, speed: 2.45, accuracy: 75.00 %\n",
            "[ 1170] loss: 7.411, speed: 2.44, accuracy: 73.75 %\n",
            "[ 1180] loss: 6.521, speed: 2.45, accuracy: 75.62 %\n",
            "[ 1190] loss: 6.918, speed: 2.45, accuracy: 74.69 %\n",
            "[ 1200] loss: 7.958, speed: 2.45, accuracy: 70.31 %\n",
            "[ 1210] loss: 7.204, speed: 2.45, accuracy: 76.56 %\n",
            "[ 1220] loss: 7.249, speed: 2.45, accuracy: 72.50 %\n",
            "[ 1230] loss: 6.822, speed: 2.45, accuracy: 74.06 %\n",
            "[ 1240] loss: 7.937, speed: 2.45, accuracy: 70.94 %\n",
            "[ 1250] loss: 6.728, speed: 2.45, accuracy: 76.56 %\n",
            "[ 1260] loss: 6.809, speed: 2.45, accuracy: 73.44 %\n",
            "[ 1270] loss: 7.282, speed: 2.45, accuracy: 72.81 %\n",
            "[ 1280] loss: 7.671, speed: 2.45, accuracy: 73.12 %\n",
            "[ 1290] loss: 7.779, speed: 2.45, accuracy: 71.25 %\n",
            "[ 1300] loss: 6.154, speed: 2.46, accuracy: 79.69 %\n",
            "[ 1310] loss: 6.547, speed: 2.45, accuracy: 76.88 %\n",
            "[ 1320] loss: 5.999, speed: 2.45, accuracy: 76.88 %\n",
            "[ 1330] loss: 6.804, speed: 2.46, accuracy: 76.25 %\n",
            "[ 1340] loss: 6.989, speed: 2.45, accuracy: 76.25 %\n",
            "[ 1350] loss: 7.103, speed: 2.45, accuracy: 72.81 %\n",
            "[ 1360] loss: 6.261, speed: 2.46, accuracy: 76.56 %\n",
            "[ 1370] loss: 6.468, speed: 2.46, accuracy: 76.56 %\n",
            "[ 1380] loss: 7.854, speed: 2.46, accuracy: 74.69 %\n",
            "[ 1390] loss: 5.827, speed: 2.46, accuracy: 77.19 %\n",
            "[ 1400] loss: 6.325, speed: 2.46, accuracy: 77.81 %\n",
            "[ 1410] loss: 6.820, speed: 2.46, accuracy: 74.06 %\n",
            "[ 1420] loss: 6.944, speed: 2.46, accuracy: 75.94 %\n",
            "[ 1430] loss: 7.055, speed: 2.46, accuracy: 75.31 %\n",
            "[ 1440] loss: 6.681, speed: 2.46, accuracy: 77.81 %\n",
            "[ 1450] loss: 7.444, speed: 2.46, accuracy: 73.12 %\n",
            "[ 1460] loss: 7.368, speed: 2.46, accuracy: 73.75 %\n",
            "[ 1470] loss: 7.737, speed: 2.46, accuracy: 73.75 %\n",
            "[ 1480] loss: 6.256, speed: 2.46, accuracy: 76.56 %\n",
            "[ 1490] loss: 7.097, speed: 2.46, accuracy: 74.38 %\n",
            "[ 1500] loss: 5.862, speed: 2.46, accuracy: 78.75 %\n",
            "[ 1510] loss: 6.746, speed: 2.46, accuracy: 75.31 %\n",
            "[ 1520] loss: 5.880, speed: 2.46, accuracy: 78.44 %\n",
            "[ 1530] loss: 6.769, speed: 2.46, accuracy: 73.12 %\n",
            "[ 1540] loss: 7.344, speed: 2.46, accuracy: 74.06 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79QnkE-UUySP"
      },
      "source": [
        "Congrats! You successfully trained an image classification model while streaming data directly from the cloud! 🎉"
      ]
    }
  ]
}